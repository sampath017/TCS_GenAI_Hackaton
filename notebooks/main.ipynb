{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65d7711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 1. Setup & Imports\n",
    "# ================================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc656b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# 2. Load Environment & Login\n",
    "# ================================\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token)\n",
    "\n",
    "models_root = Path(\"models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# 3. Load Embedding Model\n",
    "# ================================\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    cache_folder=str(models_root),\n",
    "    model_kwargs={\"device\": \"auto\"}  # force CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/TCS_GenAI_Hackaton/.venv/lib/python3.12/site-packages/accelerate/utils/modeling.py:1582: UserWarning: Current model requires 33282 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab2f2d69090440d8e9a668b219d2019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# 4. Load LLM (Gemma-3-4B-IT)\n",
    "# ================================\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=\"auto\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "llm_wrapper = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0f1d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 5. Prepare Document Store (example PDF)\n",
    "# ================================\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"/home/TCS_GenAI_Hackaton/SampathKovvaliResume.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Build FAISS vector DB\n",
    "db = FAISS.from_documents(docs, embedding_model)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7593d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# 6. Build RetrievalQA\n",
    "# ================================\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm_wrapper,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbf5cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# 7. Ask Questions\n",
    "# ================================\n",
    "query = \"What document is it?\"\n",
    "result = qa.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf81a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Answer ---\n",
      " Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      "SAMPA TH KOVV ALI\n",
      "Data Scientist\n",
      "\u000091 9441682374 sampathkovvali@gmail.com github.com/sampath017 India\n",
      "Summary\n",
      "I am a Data Scientist with a firm background in Electrical and Electronics Engineering, equipped with robust programming and analytical \n",
      "skills. My current role at TCS involves developing AI solutions that enhance workflow efficiency and automate processes. I thrive on \n",
      "challenges and enjoy collaborating with teams to push innovation forward, building advanced applications that drive tangible results.\n",
      "Experience\n",
      "Tata Consultancy Services \u0000TCS\u0000 India\n",
      "Data Scientist 01/2023 \u0000 Present\n",
      "Leading IT services and consulting company.\n",
      "Built a RAG application using LLaMA 3.2 11B Vision model for incident classification & resolution.\n",
      "Implemented an LLM-based MOM generator for Teams meetings.\n",
      "Automated 5\u0000 workflows with Python, Selenium, and FastAPI.\n",
      "Collaborated with cross-functional teams to enhance AI-driven features using state-of-the-art GenAI techniques.\n",
      "Education\n",
      "\n",
      "Education\n",
      "Raghu Engineering College Visakhapatnam\n",
      "Bachelor of Technology 07/2018 \u0000 07/2022\n",
      "Skills\n",
      "Programming Languages Python, Pandas, NumPy, Java, Node.js\n",
      "Frameworks & Libraries Flask, FastAPI, Pytorch, ReactJS\n",
      "Automation & Scripting Batch Scripting with UNIX Commands, Selenium with python, GenAI Techniques\n",
      "Databases SQL, PostgreSQL\n",
      "Tools & Technologies Docker, Linux\n",
      "Projects\n",
      "QuickAI Remote\n",
      "01/2023 \u0000 Present\n",
      "Project aimed at improving the efficiency of deep learning model management.\n",
      "Developed a lightweight PyTorch framework to streamline the training and logging of deep learning models.\n",
      "Showcased versatility in modifying model parameters for optimized performance.\n",
      "GPT Remote\n",
      "01/2023 \u0000 Present\n",
      "A platform for testing and training transformer models.\n",
      "Created a playground for experimenting with various transformer models.\n",
      "Included features for text generation and modification.\n",
      "Interests\n",
      "Artificial Intelligence Enthusiast\n",
      "Dedicated to exploring artificial\n",
      "\n",
      "Interests\n",
      "Artificial Intelligence Enthusiast\n",
      "Dedicated to exploring artificial \n",
      "intelligence and its applications in \n",
      "transforming industries.\n",
      "Psychology Interest\n",
      "Enjoys understanding psychological \n",
      "principles to enhance personal well-\n",
      "being.\n",
      "Fitness Advocate\n",
      "Committed to maintaining fitness \n",
      "through regular exercising and healthy \n",
      "living.\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "* k s\n",
      "\n",
      "Question: What document is it?\n",
      "Helpful Answer: Resume\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Answer ---\\n\", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923dee8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcs-genai-hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
