{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d7711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Installed packages\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "\n",
    "# Custome modules\n",
    "sys.path.append(Path(\"../src\").resolve().as_posix())\n",
    "import settings as s\n",
    "from indexeing import get_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc656b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38cb1670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='thenlper/gte-large', cache_folder='/home/TCS_GenAI_Hackaton/models', model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    cache_folder=str(s.models_root_path),\n",
    ")\n",
    "\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c691a107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95055a7ca8584f458091268edccdbf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (model): Gemma3Model(\n",
       "    (vision_tower): SiglipVisionModel(\n",
       "      (vision_model): SiglipVisionTransformer(\n",
       "        (embeddings): SiglipVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "          (position_embedding): Embedding(4096, 1152)\n",
       "        )\n",
       "        (encoder): SiglipEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-26): 27 x SiglipEncoderLayer(\n",
       "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (self_attn): SiglipAttention(\n",
       "                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): SiglipMLP(\n",
       "                (activation_fn): PytorchGELUTanh()\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    )\n",
       "    (language_model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-33): 34 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=s.models_root_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=\"auto\",\n",
    "    cache_dir=s.models_root_path\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1651d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StopOnNewline(StoppingCriteria):\n",
    "#     def __call__(self, input_ids, scores, **kwargs):\n",
    "#         return input_ids[0][-1] == tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# stopping_criteria = StoppingCriteriaList([StopOnNewline()])\n",
    "\n",
    "# output = model.generate(\n",
    "#     **input_tokens,\n",
    "#     temperature=0.7,   # creativity control: 0 = deterministic, higher = more creative\n",
    "#     top_p=0.9,         # nucleus sampling\n",
    "#     do_sample=True,\n",
    "#     stopping_criteria=stopping_criteria,\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261f91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7ff2014e07d0>, model_id='google/gemma-3-4b-it')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "llm_wrapper = HuggingFacePipeline(pipeline=pipe)\n",
    "llm_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f1d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already embedded and indexed: GPT3_paper.pdf\n",
      "File already embedded and indexed: SampathKovvaliResume.pdf\n",
      "File already embedded and indexed: attention_is_all_you_need.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x7ff2014e2b40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = get_db(embedding_model)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf5cf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question using the context below.\n",
      "Return only the final answer, do NOT include any context or explanations.\n",
      "\n",
      "Context:\n",
      "Name Split Metric N Acc/F1/BLEU\n",
      "Total\n",
      "Count\n",
      "Dirty\n",
      "Acc/F1/BLEU\n",
      "Dirty\n",
      "Count\n",
      "Clean\n",
      "Acc/F1/BLEU\n",
      "Clean\n",
      "Count\n",
      "Clean\n",
      "Percentage\n",
      "Relative\n",
      "Difference\n",
      "Clean vs All\n",
      "Quac dev f1 13 44.3 7353 44.3 7315 54.1 38 1% 20%\n",
      "SQuADv2 dev f1 13 69.8 11873 69.9 11136 68.4 737 6% -2%\n",
      "DROP dev f1 13 36.5 9536 37.0 8898 29.5 638 7% -21%\n",
      "Symbol Insertion dev acc 7 66.9 10000 66.8 8565 67.1 1435 14% 0%\n",
      "CoQa dev f1 13 86.0 7983 85.3 5107 87.1 2876 36% 1%\n",
      "ReCoRD dev acc 13 89.5 10000 90.3 6110 88.2 3890 39% -1%\n",
      "Winograd test acc 9 88.6 273 90.2 164 86.2 109 40% -3%\n",
      "BoolQ dev acc 13 76.0 3270 75.8 1955 76.3 1315 40% 0%\n",
      "MultiRC dev acc 13 74.2 953 73.4 558 75.3 395 41% 1%\n",
      "RACE-h test acc 13 46.8 3498 47.0 1580 46.7 1918 55% 0%\n",
      "LAMBADA test acc 13 86.4 5153 86.9 2209 86.0 2944 57% 0%\n",
      "LAMBADA (No Blanks) test acc 13 77.8 5153 78.5 2209 77.2 2944 57% -1%\n",
      "WSC dev acc 13 76.9 104 73.8 42 79.0 62 60% 3%\n",
      "PIQA dev acc 8 82.3 1838 89.9 526 79.3 1312 71% -4%\n",
      "RACE-m test acc 13 58.5 1436 53.0 366 60.4 1070 75% 3%\n",
      "De→En 16 test bleu-sb 12 43.0 2999 47.4 739 40.8 2260 75% -5%\n",
      "En→De 16 test bleu-sb 12 30.9 2999 32.6 739 29.9 2260 75% -3%\n",
      "En→Ro 16 test bleu-sb 12 25.8 1999 24.9 423 26.1 1576 79% 1%\n",
      "Ro→En 16 test bleu-sb 12 41.3 1999 40.4 423 41.6 1576 79% 1%\n",
      "WebQs test acc 8 41.5 2032 41.6 428 41.5 1604 79% 0%\n",
      "ANLI R1 test acc 13 36.8 1000 40.5 200 35.9 800 80% -3%\n",
      "ANLI R2 test acc 13 34.0 1000 29.4 177 35.0 823 82% 3%\n",
      "TriviaQA dev acc 10 71.2 7993 70.8 1390 71.3 6603 83% 0%\n",
      "ANLI R3 test acc 13 40.2 1200 38.3 196 40.5 1004 84% 1%\n",
      "En→Fr 14 test bleu-sb 13 39.9 3003 38.3 411 40.3 2592 86% 1%\n",
      "Fr→En 14 test bleu-sb 13 41.4 3003 40.9 411 41.4 2592 86% 0%\n",
      "WiC dev acc 13 51.4 638 53.1 49 51.3 589 92% 0%\n",
      "RTE dev acc 13 71.5 277 71.4 21 71.5 256 92% 0%\n",
      "CB dev acc 13 80.4 56 100.0 4 78.8 52 93% -2%\n",
      "Anagrams 2 dev acc 2 40.2 10000 76.2 705 37.4 9295 93% -7%\n",
      "Reversed Words dev acc 2 0.4 10000 1.5 660 0.3 9340 93% -26%\n",
      "OpenBookQA test acc 8 65.4 500 58.1 31 65.9 469 94% 1%\n",
      "\n",
      "WSC acc dev 93.8 32 59.6 56.7 65.4 61.5 66.3 60.6 64.4 65.4 58.7 58.7 60.6 62.5 66.3 60.6 66.3 69.2 58.7 60.6 54.8 49.0 62.5 67.3 75.0 75.0 80.1\n",
      "MultiRC acc dev 62.3 32 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 6.09 11.8 16.8 20.8 24.7 23.8 25.0 32.5 30.5\n",
      "MultiRC f1a dev 88.2 32 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 45.0 55.9 64.2 65.4 69.5 66.4 69.3 74.8 75.4\n",
      "ReCoRD acc dev 92.5 32 70.8 78.5 82.1 84.1 86.2 88.6 89.0 90.2 69.8 77.0 80.7 83.0 85.9 88.0 88.8 90.2 69.8 77.2 81.3 83.1 86.6 87.9 88.9 89.0 90.2\n",
      "ReCoRD f1 dev 93.3 32 71.9 79.2 82.8 85.2 87.3 89.5 90.4 91.0 70.7 77.8 81.6 83.9 86.8 88.8 89.7 91.2 70.7 77.9 82.1 84.0 87.5 88.8 89.8 90.1 91.1\n",
      "SuperGLUE average dev 89.0 40.6 47.4 46.8 49.6 50.1 52.3 54.4 58.2 54.4 55.1 56.7 57.8 61.2 59.7 64.3 68.9 50.2 56.2 56.8 60.0 64.3 63.6 66.9 73.2 71.8\n",
      "ANLI R1 acc test 73.8 50 33.4 34.2 33.4 33.4 34.2 32.3 33.2 34.6 32.1 31.6 31.9 34.6 30.6 31.6 32.7 32.0 32.1 32.5 30.9 32.5 33.5 33.1 33.3 36.8\n",
      "ANLI R2 acc test 50.7 50 33.2 31.9 33.3 33.3 33.8 33.5 33.5 35.4 35.7 33.7 33.2 32.7 32.7 33.9 33.9 33.9 35.7 33.8 32.1 31.4 32.6 33.3 32.6 34.0\n",
      "ANLI R3 acc test 48.3 50 33.6 34.0 33.8 33.4 35.3 34.8 34.4 34.5 35.0 32.6 33.0 33.9 34.1 33.1 32.5 35.1 35.0 34.4 35.1 36.0 32.7 33.9 34.5 40.2\n",
      "2D+ acc n/a 50 0.70 0.65 0.70 0.85 1.10 2.54 15.4 76.9 2.00 0.55 3.15 4.00 12.1 19.6 73.0 99.6 2.00 4.10 3.50 4.50 8.90 11.9 55.5 100.0\n",
      "2D- acc n/a 50 1.25 1.25 1.25 1.25 1.60 7.60 12.6 58.0 1.15 0.95 1.45 1.95 3.85 11.5 44.6 86.4 1.15 1.45 2.25 2.70 7.35 13.6 52.4 98.9\n",
      "3D+ acc n/a 50 0.10 0.10 0.05 0.10 0.10 0.25 1.40 34.2 0.15 0.00 0.10 0.30 0.45 0.95 15.4 65.5 0.15 0.45 0.30 0.55 0.75 0.90 8.40 80.4\n",
      "3D- acc n/a 50 0.05 0.05 0.05 0.05 0.05 0.45 1.35 48.3 0.05 0.15 0.25 0.30 0.55 1.60 6.15 78.7 0.05 0.10 0.15 0.35 0.65 1.05 9.20 94.2\n",
      "\n",
      "4D+ acc n/a 50 0.05 0.05 0.00 0.00 0.05 0.05 0.15 4.00 0.00 0.00 0.10 0.00 0.00 0.10 0.80 14.0 0.00 0.05 0.05 0.00 0.15 0.15 0.40 25.5\n",
      "4D- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.10 7.50 0.00 0.00 0.00 0.00 0.05 0.00 0.50 14.0 0.00 0.05 0.00 0.00 0.10 0.05 0.40 26.8\n",
      "5D+ acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.45 0.00 0.00 0.00 0.00 0.00 0.00 0.05 9.30\n",
      "5D- acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.90\n",
      "2Dx acc n/a 50 2.20 2.25 2.65 2.10 2.55 5.80 6.15 19.8 1.35 2.35 3.35 2.35 4.75 9.15 11.0 27.4 1.35 2.90 2.70 2.85 4.25 6.10 7.05 29.2\n",
      "1DC acc n/a 50 1.25 2.95 2.75 0.05 0.30 2.35 0.75 9.75 1.90 2.80 2.85 3.65 6.45 9.15 8.20 14.3 1.70 2.15 3.90 5.75 6.20 7.60 9.95 21.3\n",
      "Cycled Letters acc n/a 100 0.62 0.71 2.85 0.00 0.63 1.35 2.58 3.66 1.67 4.36 5.68 6.46 6.25 9.41 15.1 21.7 4.63 9.27 10.7 14.5 16.7 21.9 27.7 37.9\n",
      "Anagrams 1 acc n/a 100 0.10 0.14 0.40 0.00 0.27 0.69 1.16 2.28 0.21 0.61 1.12 1.27 1.60 2.72 3.72 8.62 0.50 1.27 2.13 3.05 3.81 5.49 8.38 15.1\n",
      "Anagrams 2 acc n/a 100 0.81 1.21 2.69 0.01 1.71 3.75 4.53 8.91 1.19 2.62 4.70 4.77 6.97 10.2 14.6 25.9 1.94 4.80 7.59 9.87 12.6 18.9 25.6 39.7\n",
      "Symbol Insertion acc n/a 100 0.00 0.00 0.10 0.00 0.05 0.42 0.89 8.26 0.03 0.05 0.57 1.18 1.67 3.46 6.62 45.4 0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2\n",
      "Reversed Words acc n/a 100 0.00 0.01 0.01 0.01 0.02 0.03 0.03 0.09 0.02 0.01 0.01 0.00 0.05 0.07 0.11 0.48 0.00 0.05 0.00 0.17 0.24 0.30 0.42 0.44\n",
      "SAT Analogies acc n/a 20 35.6 39.0 45.2 44.1 50.0 49.2 52.7 53.7 30.5 41.2 43.1 46.5 55.1 54.3 53.5 59.1 30.5 40.4 42.8 40.6 48.4 51.9 53.5 65.2\n",
      "Table H.1: Scores for every task, setting and model that we investigate in this paper.\n",
      "63\n",
      "\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      "SAMPA TH KOVV ALI\n",
      "Data Scientist\n",
      "\u000091 9441682374 sampathkovvali@gmail.com github.com/sampath017 India\n",
      "Summary\n",
      "I am a Data Scientist with a firm background in Electrical and Electronics Engineering, equipped with robust programming and analytical \n",
      "skills. My current role at TCS involves developing AI solutions that enhance workflow efficiency and automate processes. I thrive on \n",
      "challenges and enjoy collaborating with teams to push innovation forward, building advanced applications that drive tangible results.\n",
      "Experience\n",
      "Tata Consultancy Services \u0000TCS\u0000 India\n",
      "Data Scientist 01/2023 \u0000 Present\n",
      "Leading IT services and consulting company.\n",
      "Built a RAG application using LLaMA 3.2 11B Vision model for incident classification & resolution.\n",
      "Implemented an LLM-based MOM generator for Teams meetings.\n",
      "Automated 5\u0000 workflows with Python, Selenium, and FastAPI.\n",
      "Collaborated with cross-functional teams to enhance AI-driven features using state-of-the-art GenAI techniques.\n",
      "Education\n",
      "Raghu Engineering College Visakhapatnam\n",
      "Bachelor of Technology 07/2018 \u0000 07/2022\n",
      "Skills\n",
      "Programming Languages Python, Pandas, NumPy, Java, Node.js\n",
      "Frameworks & Libraries Flask, FastAPI, Pytorch, ReactJS\n",
      "Automation & Scripting Batch Scripting with UNIX Commands, Selenium with python, GenAI Techniques\n",
      "Databases SQL, PostgreSQL\n",
      "Tools & Technologies Docker, Linux\n",
      "Projects\n",
      "QuickAI Remote\n",
      "01/2023 \u0000 Present\n",
      "Project aimed at improving the efficiency of deep learning model management.\n",
      "Developed a lightweight PyTorch framework to streamline the training and logging of deep learning models.\n",
      "Showcased versatility in modifying model parameters for optimized performance.\n",
      "GPT Remote\n",
      "01/2023 \u0000 Present\n",
      "A platform for testing and training transformer models.\n",
      "Created a playground for experimenting with various transformer models.\n",
      "Included features for text generation and modification.\n",
      "Interests\n",
      "Artificial Intelligence Enthusiast\n",
      "Dedicated to exploring artificial \n",
      "intelligence and its applications in\n",
      "\n",
      "Question:\n",
      "what is my aadhar number?\n",
      "\n",
      "Answer:\n",
      "1682374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Answer the question using the context below.\n",
    "Return only the final answer, do NOT include any context or explanations.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Create retrieval chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm_wrapper,\n",
    "    chain_type=\"stuff\",  # or \"map_reduce\" for large docs\n",
    "    retriever=db.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 3️⃣ Pass your query\n",
    "query = \"what is my aadhar number?\"\n",
    "result = qa.invoke(query)  # or qa.invoke(query)\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "923dee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682374\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"].split(\"Answer:\\n\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdda495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcs-genai-hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
