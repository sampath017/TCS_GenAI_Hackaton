{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d7711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Installed packages\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.agents import create_sql_agent\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Custome modules\n",
    "sys.path.append(Path(\"../src\").resolve().as_posix())\n",
    "import settings as s\n",
    "from indexeing import get_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc656b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38cb1670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='thenlper/gte-large', cache_folder='C:\\\\Users\\\\sampath\\\\Dev\\\\TCS_GenAI_Hackaton\\\\models', model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    cache_folder=str(s.models_root_path),\n",
    ")\n",
    "\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c691a107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522377c6b9ee4d88b4c87cd7734f056c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (model): Gemma3Model(\n",
       "    (vision_tower): SiglipVisionModel(\n",
       "      (vision_model): SiglipVisionTransformer(\n",
       "        (embeddings): SiglipVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "          (position_embedding): Embedding(4096, 1152)\n",
       "        )\n",
       "        (encoder): SiglipEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-26): 27 x SiglipEncoderLayer(\n",
       "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (self_attn): SiglipAttention(\n",
       "                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): SiglipMLP(\n",
       "                (activation_fn): PytorchGELUTanh()\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    )\n",
       "    (language_model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-33): 34 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=s.models_root_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=\"auto\",\n",
    "    cache_dir=s.models_root_path\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261f91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x0000025730283E30>, model_id='google/gemma-3-4b-it')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(f\"sqlite:///{s.db_path}\")\n",
    "\n",
    "# Create toolkit\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "# Create SQL Agent\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=llm,\n",
    "    toolkit=toolkit,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the total quantity of finished goods?\"\n",
    "agent_executor.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cdda495",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 1. Load CSV into SQLite\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     10\u001b[39m db = SQLDatabase.from_uri(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msqlite:///\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms.db_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GPU\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m llm = HuggingFacePipeline(pipeline=pipe)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 4. Define Tools\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sampath\\Dev\\TCS_GenAI_Hackaton\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1230\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1228\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mprocessor\u001b[39m\u001b[33m\"\u001b[39m] = processor\n\u001b[32m-> \u001b[39m\u001b[32m1230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sampath\\Dev\\TCS_GenAI_Hackaton\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:121\u001b[39m, in \u001b[36mTextGenerationPipeline.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_model_type(\n\u001b[32m    123\u001b[39m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[32m    124\u001b[39m     )\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._preprocess_params:\n\u001b[32m    126\u001b[39m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[32m    127\u001b[39m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sampath\\Dev\\TCS_GenAI_Hackaton\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:981\u001b[39m, in \u001b[36mPipeline.__init__\u001b[39m\u001b[34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, device, binary_output, **kwargs)\u001b[39m\n\u001b[32m    978\u001b[39m hf_device_map = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mhf_device_map\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    982\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    983\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdiscard the `device` argument when creating your pipeline object.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    984\u001b[39m     )\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Take the first device used by `accelerate`.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object."
     ]
    }
   ],
   "source": [
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load CSV into SQLite\n",
    "# -----------------------\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{s.db_path}\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,  # GPU\n",
    "    max_new_tokens=512,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# -----------------------\n",
    "# 4. Define Tools\n",
    "# -----------------------\n",
    "def run_sql(query: str):\n",
    "    return db.run(query)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"sql_db_query\",\n",
    "        func=run_sql,\n",
    "        description=\"Run SQL queries on the inventory database\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# 5. Agent with Tool Calling\n",
    "# -----------------------\n",
    "agent_executor = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",  # ReAct agent with tool use\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 6. Query Example\n",
    "# -----------------------\n",
    "query = \"What is the total quantity of finished goods?\"\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(\"Answer:\", response[\"output\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcs-genai-hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
