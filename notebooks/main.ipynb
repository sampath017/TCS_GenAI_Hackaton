{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d7711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Installed packages\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "\n",
    "# Custome modules\n",
    "sys.path.append(Path(\"../src\").resolve().as_posix())\n",
    "import settings as s\n",
    "from indexeing import get_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc656b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38cb1670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='thenlper/gte-large', cache_folder='/home/TCS_GenAI_Hackaton/models', model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-large\",\n",
    "    cache_folder=str(s.models_root_path),\n",
    ")\n",
    "\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c691a107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c3315cda6d444e995fddbf622f8fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (model): Gemma3Model(\n",
       "    (vision_tower): SiglipVisionModel(\n",
       "      (vision_model): SiglipVisionTransformer(\n",
       "        (embeddings): SiglipVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "          (position_embedding): Embedding(4096, 1152)\n",
       "        )\n",
       "        (encoder): SiglipEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-26): 27 x SiglipEncoderLayer(\n",
       "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (self_attn): SiglipAttention(\n",
       "                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              (mlp): SiglipMLP(\n",
       "                (activation_fn): PytorchGELUTanh()\n",
       "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "    )\n",
       "    (language_model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-33): 34 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=s.models_root_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=\"auto\",\n",
    "    cache_dir=s.models_root_path\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1651d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StopOnNewline(StoppingCriteria):\n",
    "#     def __call__(self, input_ids, scores, **kwargs):\n",
    "#         return input_ids[0][-1] == tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# stopping_criteria = StoppingCriteriaList([StopOnNewline()])\n",
    "\n",
    "# output = model.generate(\n",
    "#     **input_tokens,\n",
    "#     temperature=0.7,   # creativity control: 0 = deterministic, higher = more creative\n",
    "#     top_p=0.9,         # nucleus sampling\n",
    "#     do_sample=True,\n",
    "#     stopping_criteria=stopping_criteria,\n",
    "# )\n",
    "\n",
    "# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "261f91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fdc04bef560>, model_id='google/gemma-3-4b-it')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "llm_wrapper = HuggingFacePipeline(pipeline=pipe)\n",
    "llm_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f1d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed and indexed: bill_of_materials.csv\n",
      "File processed and indexed: finished_goods_inventory.csv\n",
      "File processed and indexed: production_schedule.csv\n",
      "File processed and indexed: purchases.csv\n",
      "File processed and indexed: raw_material_inventory.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x7fdc04bedd00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = get_db(embedding_model)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbf5cf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What products are available in finished goods inventory search in the file finished_goods_inventory.csv?\n",
      "A: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "item_id: 25\n",
      "item_name: Acetone\n",
      "category: Raw Material\n",
      "stock_level: 579\n",
      "reorder_point: 640\n",
      "lead_time_days: 7\n",
      "supplier: SteelCorp\n",
      "\n",
      "item_id: 25\n",
      "item_name: Acetone\n",
      "category: Raw Material\n",
      "stock_level: 579\n",
      "reorder_point: 640\n",
      "lead_time_days: 7\n",
      "supplier: SteelCorp\n",
      "\n",
      "item_id: 13\n",
      "item_name: PVC Pipes\n",
      "category: Raw Material\n",
      "stock_level: 668\n",
      "reorder_point: 997\n",
      "lead_time_days: 9\n",
      "supplier: BoxMakers\n",
      "\n",
      "Question: What products are available in finished goods inventory search in the file finished_goods_inventory.csv?\n",
      "Helpful Answer: The file contains the following information: item_id, item_name, category, stock_level, and finished_goods_flag. The question asks for products available in finished goods inventory. Thus, we need to filter the data to find the rows where the 'finished_goods_flag' column is True.\n",
      "```csv\n",
      "item_id,item_name,category,stock_level,finished_goods_flag\n",
      "1,Widget A,Finished Goods,100,True\n",
      "2,Widget B,Finished Goods,50,True\n",
      "3,Widget C,Raw Material,200,False\n",
      "4,Widget D,Finished Goods,75,True\n",
      "5,Widget E,Raw Material,150,False\n",
      "```\n",
      "The products available in finished goods inventory are Widget A, Widget B, and Widget D.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "conversational_qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm_wrapper,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # optional: see where answer came from\n",
    ")\n",
    "\n",
    "chat_history = []  # stores dialog\n",
    "\n",
    "query1 = \"What products are available in finished goods inventory search in the file finished_goods_inventory.csv?\"\n",
    "result1 = conversational_qa.invoke({\"question\": query1, \"chat_history\": chat_history})\n",
    "print(\"Q:\", query1)\n",
    "print(\"A:\", result1[\"answer\"])\n",
    "\n",
    "# # Add to history\n",
    "# chat_history.append((query1, result1[\"answer\"]))\n",
    "\n",
    "# query2 = \"And how many raw materials do we have?\"\n",
    "# result2 = conversational_qa.invoke({\"question\": query2, \"chat_history\": chat_history})\n",
    "# print(\"Q:\", query2)\n",
    "# print(\"A:\", result2[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdda495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcs-genai-hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
